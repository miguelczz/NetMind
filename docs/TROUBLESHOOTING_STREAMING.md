# Soluci√≥n de Problemas - Streaming y Contexto

## üîß Cambios Realizados

### 1. Streaming Visual Mejorado

**Archivo:** `backend/src/api/streaming.py`

**Cambios:**
- ‚úÖ Reducido `chunk_size` de 15 a **5 caracteres** (m√°s fluido)
- ‚úÖ Aumentado delay de 20ms a **50ms** (m√°s visible)
- ‚úÖ Implementado pseudo-streaming dividiendo la respuesta final

**Resultado esperado:**
- Deber√≠as ver el texto aparecer palabra por palabra (aproximadamente 20 caracteres por segundo)
- El efecto visual es similar a ChatGPT

---

## üß™ C√≥mo Probar el Streaming

### Paso 1: Reiniciar el Backend

```bash
# Detener el backend actual (Ctrl+C)
# Luego reiniciar:
cd backend
python -m uvicorn main:app --reload
```

### Paso 2: Reiniciar el Frontend

```bash
# Detener el frontend actual (Ctrl+C)
# Luego reiniciar:
cd frontend
npm run dev
```

### Paso 3: Probar con una Pregunta Simple

```
¬øQu√© es TCP/IP?
```

**Resultado esperado:**
- Deber√≠as ver una burbuja vac√≠a aparecer inmediatamente
- El texto deber√≠a ir apareciendo palabra por palabra
- Velocidad: aproximadamente 20 caracteres por segundo

### Paso 4: Verificar en la Consola del Navegador

Abre las DevTools (F12) y ve a la pesta√±a "Console". Deber√≠as ver logs como:

```
Recibiendo chunk SSE: {type: "token", data: {content: "El pr"}}
Recibiendo chunk SSE: {type: "token", data: {content: "otoco"}}
Recibiendo chunk SSE: {type: "token", data: {content: "lo TC"}}
...
```

---

## üêõ Problema del Contexto Incorrecto

### Diagn√≥stico

El problema que reportaste:
- **Esperado:** Latencia promedio de 77.29 ms
- **Obtenido:** Latencia promedio de 30 ms

**Causa ra√≠z:** El LLM est√° extrayendo incorrectamente los datos num√©ricos del contexto de conversaci√≥n.

### Soluci√≥n

El c√≥digo ya tiene l√≥gica para usar el contexto de conversaci√≥n (l√≠neas 588-605 en `tool_executors.py`), pero el prompt necesita ser m√°s espec√≠fico para extraer datos num√©ricos exactos.

**Archivo a modificar:** `backend/src/agent/tool_executors.py`

**L√≠nea 588-605:** Modificar el prompt para ser m√°s estricto con datos num√©ricos:

```python
followup_prompt = f"""
Bas√°ndote en la siguiente conversaci√≥n previa, responde la pregunta del usuario de forma DIRECTA y PRECISA.

IMPORTANTE:
- Si la pregunta es sobre DATOS NUM√âRICOS (latencia, tiempo, porcentaje, etc.), extrae los valores EXACTOS del contexto
- NO inventes ni aproximes n√∫meros - usa SOLO los valores que aparecen en el contexto
- Si hay m√∫ltiples valores, usa el m√°s reciente o el m√°s relevante seg√∫n la pregunta
- S√© CONCISO: ve directo al punto

Conversaci√≥n previa:
{context}

Pregunta del usuario: {user_prompt}

Respuesta (directa, precisa, con valores exactos del contexto):
"""
```

---

## üîç Debugging Paso a Paso

### Si el Streaming NO Funciona

1. **Verificar que el backend est√© usando el endpoint correcto:**
   - Abre DevTools ‚Üí Network
   - Env√≠a un mensaje
   - Busca la petici√≥n a `/agent/query/stream`
   - Deber√≠a ser tipo "eventsource" o "fetch"

2. **Verificar que los chunks lleguen al frontend:**
   - Abre DevTools ‚Üí Console
   - Deber√≠as ver logs de chunks recibidos
   - Si no ves logs, el problema est√° en el frontend

3. **Verificar que el backend est√© enviando chunks:**
   - Revisa los logs del backend
   - Deber√≠as ver: `[Streaming] Respuesta guardada en contexto de ventana`

### Si el Contexto Est√° Mal

1. **Verificar que el contexto se est√© guardando:**
   - Env√≠a un mensaje
   - Revisa los logs del backend
   - Deber√≠as ver: `[Streaming] Respuesta guardada en contexto de ventana para sesi√≥n XXX`

2. **Verificar que el contexto se est√© usando:**
   - Env√≠a una pregunta de seguimiento
   - Revisa los logs del backend
   - Deber√≠as ver: `[RAG] Detectado seguimiento de conversaci√≥n - usando contexto de conversaci√≥n`

3. **Verificar el contenido del contexto:**
   - Agrega un log temporal en `tool_executors.py` l√≠nea 599:
   ```python
   logger.info(f"[DEBUG] Contexto usado: {context[:500]}")  # Primeros 500 chars
   ```
   - Verifica que el contexto contenga los datos correctos del ping anterior

---

## üéØ Soluci√≥n R√°pida para el Contexto

Si quieres una soluci√≥n inmediata para el problema del contexto, puedes hacer lo siguiente:

### Opci√≥n 1: Mejorar el Prompt (Recomendado)

Modifica el prompt en `tool_executors.py` l√≠nea 588 como se indic√≥ arriba.

### Opci√≥n 2: Aumentar el Contexto

Aumenta el n√∫mero de mensajes en el contexto:

```python
# L√≠nea 582
context_text = get_conversation_context(messages, max_messages=15)  # Era 10
```

### Opci√≥n 3: Agregar Validaci√≥n de Datos Num√©ricos

Agrega validaci√≥n para asegurar que los datos num√©ricos se extraigan correctamente:

```python
# Despu√©s de generar la respuesta (l√≠nea 605)
answer = generate_from_context(context_text, prompt)

# Validar que los n√∫meros en la respuesta coincidan con los del contexto
import re
numbers_in_context = re.findall(r'\d+\.?\d*\s*ms', context_text)
numbers_in_answer = re.findall(r'\d+\.?\d*\s*ms', answer)

if numbers_in_context and not numbers_in_answer:
    logger.warning(f"[RAG] Respuesta no contiene datos num√©ricos del contexto")
    # Regenerar con prompt m√°s espec√≠fico
```

---

## üìä M√©tricas de √âxito

### Streaming Funcionando Correctamente
- ‚úÖ Texto aparece palabra por palabra
- ‚úÖ Velocidad visible pero no molesta (~20 chars/seg)
- ‚úÖ Solo UNA burbuja de respuesta
- ‚úÖ No hay duplicados

### Contexto Funcionando Correctamente
- ‚úÖ Pregunta de seguimiento usa datos del mensaje anterior
- ‚úÖ Datos num√©ricos son exactos (no aproximados)
- ‚úÖ El agente "recuerda" acciones previas (pings, consultas DNS, etc.)

---

## üöÄ Pr√≥ximos Pasos

1. **Probar el streaming** con los nuevos par√°metros (chunk_size=5, delay=50ms)
2. **Verificar el contexto** con una pregunta de seguimiento
3. **Si el contexto sigue mal**, aplicar la soluci√≥n del prompt mejorado
4. **Reportar resultados** para ajustar si es necesario

---

## üí° Notas Importantes

- El streaming es **pseudo-streaming** (divide la respuesta final), no streaming real token por token del LLM
- Esto es suficiente para la experiencia de usuario y mucho m√°s simple de implementar
- El streaming real requerir√≠a refactorizar todo el c√≥digo para usar LangChain LLMs en lugar de llamadas directas a OpenAI
- El contexto ya est√° implementado, solo necesita ajustes en el prompt para extraer datos num√©ricos correctamente
