# üöÄ Optimizaciones de Tiempo de Respuesta - NetMind

Este documento detalla todas las t√©cnicas de optimizaci√≥n implementadas en el proyecto NetMind para reducir los tiempos de respuesta y mejorar la experiencia del usuario.

---

## üìã Tabla de Contenidos

1. [Streaming de Respuestas (SSE)](#1-streaming-de-respuestas-sse)
2. [Pool de Conexiones de Base de Datos](#2-pool-de-conexiones-de-base-de-datos)
3. [Procesamiento As√≠ncrono](#3-procesamiento-as√≠ncrono)
4. [B√∫squeda H√≠brida Paralela](#4-b√∫squeda-h√≠brida-paralela)
5. [Cach√© con Redis](#5-cach√©-con-redis)
6. [Pre-compilaci√≥n de Prompts](#6-pre-compilaci√≥n-de-prompts)
7. [Validaci√≥n y An√°lisis en Paralelo](#7-validaci√≥n-y-an√°lisis-en-paralelo)
8. [Optimizaciones de Configuraci√≥n](#8-optimizaciones-de-configuraci√≥n)

---

## 1. Streaming de Respuestas (SSE)

### üìç Ubicaci√≥n
- **Archivo**: `backend/src/api/streaming.py`

### üéØ Objetivo
Enviar tokens de respuesta al usuario de forma incremental mientras se generan, en lugar de esperar a que se complete toda la respuesta.

### üí° Implementaci√≥n

```20:122:backend/src/api/streaming.py
async def stream_graph_execution(
    initial_state: GraphState,
    session_id: str
) -> AsyncIterator[str]:
    """
    Ejecuta el grafo y stream los resultados usando SSE.
    OPTIMIZACI√ìN: Usa astream_events para capturar tokens del LLM en tiempo real.
    """
    try:
        # OPTIMIZACI√ìN: Usar astream_events para capturar eventos en tiempo real
        # Esto permite capturar tokens del LLM mientras se generan
        async for event in graph.astream_events(initial_state, version="v2"):
            event_type = event.get("event")
            event_name = event.get("name", "")
            
            # Capturar tokens del LLM en tiempo real (OPTIMIZACI√ìN CR√çTICA)
            if event_type == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, "content") and chunk.content:
                    token_data = {
                        "type": "token",
                        "data": {
                            "content": chunk.content
                        }
                    }
                    yield f"data: {json.dumps(token_data)}\n\n"
```

### üîë Caracter√≠sticas Clave

1. **Server-Sent Events (SSE)**: Formato est√°ndar para streaming de datos
2. **Captura de tokens en tiempo real**: Usa `astream_events` de LangGraph para capturar tokens mientras se generan
3. **Headers optimizados**:
   - `Cache-Control: no-cache`: Evita que proxies cacheen la respuesta
   - `Connection: keep-alive`: Mantiene la conexi√≥n abierta
   - `X-Accel-Buffering: no`: Desactiva buffering en nginx para env√≠o inmediato
4. **Actualizaciones de estado**: Env√≠a actualizaciones sobre el progreso del grafo (nodos ejecutados, herramientas usadas)

### üìä Beneficios

- **Time to First Token (TTFT)**: El usuario ve la respuesta casi inmediatamente
- **Percepci√≥n de velocidad**: La respuesta se siente m√°s r√°pida aunque el tiempo total sea el mismo
- **Mejor UX**: El usuario no ve una pantalla en blanco esperando
- **Transparencia**: El usuario puede ver qu√© nodos del grafo se est√°n ejecutando

### üîß Uso

```bash
curl -X POST "http://localhost:8000/agent/query/stream" \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "mi-sesion",
    "messages": [{
      "role": "user",
      "content": "¬øQu√© es un ping?"
    }]
  }'
```

---

## 2. Pool de Conexiones de Base de Datos

### üìç Ubicaci√≥n
- **Archivo**: `backend/src/models/database.py` (l√≠neas 64-73)

### üéØ Objetivo
Reutilizar conexiones de base de datos en lugar de crear nuevas para cada solicitud, reduciendo la sobrecarga de establecer conexiones.

### üí° Implementaci√≥n

```64:73:backend/src/models/database.py
# Crear engine de SQLAlchemy con pool optimizado
# OPTIMIZACI√ìN: Configurar pool de conexiones para mejor rendimiento
engine = create_engine(
    settings.sqlalchemy_url,
    pool_pre_ping=True,  # Verifica conexiones antes de usarlas
    pool_size=10,  # N√∫mero de conexiones a mantener en el pool
    max_overflow=20,  # M√°ximo de conexiones adicionales que se pueden crear
    pool_recycle=3600,  # Reciclar conexiones despu√©s de 1 hora
    echo=False  # Cambiar a True para ver queries SQL en desarrollo
)
```

### üîë Caracter√≠sticas Clave

1. **Pool de conexiones**: SQLAlchemy mantiene un conjunto de conexiones reutilizables
2. **Configuraci√≥n**:
   - `pool_size=10`: N√∫mero de conexiones a mantener en el pool
   - `max_overflow=20`: M√°ximo de conexiones adicionales que se pueden crear bajo carga
   - `pool_pre_ping=True`: Verifica conexiones antes de usarlas (detecta conexiones cerradas)
   - `pool_recycle=3600`: Recicla conexiones despu√©s de 1 hora para evitar timeouts
3. **Gesti√≥n autom√°tica**: Las conexiones se adquieren y liberan autom√°ticamente mediante el dependency `get_db()`

### üìä Beneficios

- **Reducci√≥n de latencia**: Evita el tiempo de establecer nuevas conexiones (t√≠picamente 50-200ms)
- **Mejor escalabilidad**: Maneja m√∫ltiples solicitudes concurrentes eficientemente
- **Menor carga en BD**: Reutiliza conexiones existentes
- **Detecci√≥n de conexiones muertas**: `pool_pre_ping` detecta y reemplaza conexiones cerradas autom√°ticamente

### üîß Configuraci√≥n

El pool se configura autom√°ticamente al importar el m√≥dulo. Para ajustar los valores:

```python
# En backend/src/models/database.py
engine = create_engine(
    settings.sqlalchemy_url,
    pool_size=15,      # Aumentar para m√°s concurrencia
    max_overflow=30,   # Aumentar para picos de carga
    pool_recycle=3600  # Ajustar seg√∫n timeout de BD
)
```

---

## 3. Procesamiento As√≠ncrono

### üìç Ubicaci√≥n
- Todo el proyecto usa `async/await` consistentemente
- Endpoints FastAPI son as√≠ncronos
- Operaciones de I/O usan `asyncio`

### üéØ Objetivo
Permitir que m√∫ltiples operaciones I/O (base de datos, APIs externas, etc.) se ejecuten concurrentemente en lugar de secuencialmente.

### üí° Ejemplo de Implementaci√≥n

```python
# backend/src/api/agent.py
@router.post("/query")
async def agent_query(
    query: AgentQuery,
    session_manager: SessionManager = Depends(get_session_manager)
):
    """
    Endpoint as√≠ncrono que procesa consultas del agente.
    """
    # Operaciones as√≠ncronas no bloquean el event loop
    session_state = session_manager.get_session(query.session_id, query.user_id)
    initial_state = GraphState(messages=graph_messages)
    final_state = await graph.ainvoke(initial_state)  # No bloquea otras solicitudes
    return response
```

### üìä Beneficios

- **Concurrencia**: M√∫ltiples solicitudes pueden procesarse simult√°neamente
- **No bloquea**: Mientras espera I/O, puede procesar otras solicitudes
- **Mejor utilizaci√≥n de recursos**: Aprovecha mejor el CPU y la red
- **Escalabilidad**: Puede manejar miles de solicitudes concurrentes

### üîß Principios Aplicados

```python
# ‚ùå Evitar (s√≠ncrono)
def process_data():
    result1 = db.query()  # Bloquea
    result2 = api.call()   # Bloquea
    return combine(result1, result2)

# ‚úÖ Preferir (as√≠ncrono)
async def process_data():
    result1 = await db.query()  # No bloquea, permite otras operaciones
    result2 = await api.call()  # No bloquea
    return combine(result1, result2)
```

---

## 4. B√∫squeda H√≠brida Paralela

### üìç Ubicaci√≥n
- **Archivo**: `backend/src/tools/rag_tool.py` (l√≠neas 320-368)

### üéØ Objetivo
Ejecutar b√∫squedas densas (vectoriales) y dispersas (por palabras clave) simult√°neamente en lugar de secuencialmente.

### üí° Implementaci√≥n

```320:368:backend/src/tools/rag_tool.py
    async def _execute_query(self, query_text: str, top_k: int = 8, conversation_context: Optional[str] = None):
        """
        M√©todo interno que ejecuta la consulta RAG real.
        OPTIMIZACI√ìN: B√∫squeda h√≠brida paralela (densa + dispersa) usando asyncio.gather().
        """
        # OPTIMIZACI√ìN: Extraer keywords antes de las b√∫squedas
        keywords = self._extract_keywords(query_text)
        
        # OPTIMIZACI√ìN: Ejecutar b√∫squeda densa y dispersa en paralelo usando asyncio.gather()
        # Esto es m√°s eficiente que ThreadPoolExecutor para operaciones I/O
        tasks = [self._dense_search(query_text, top_k)]
        
        # Agregar b√∫squeda dispersa solo si hay keywords
        if keywords:
            tasks.append(self._sparse_search(keywords))
        
        # Ejecutar ambas b√∫squedas en paralelo
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Procesar resultados
        hits = results[0] if not isinstance(results[0], Exception) else []
        keyword_hits = results[1] if len(results) > 1 and not isinstance(results[1], Exception) else []
```

### üîë Caracter√≠sticas Clave

1. **`asyncio.gather()`**: Ejecuta ambas b√∫squedas en paralelo
2. **Independencia**: Las b√∫squedas densa y dispersa no dependen una de otra
3. **Reducci√≥n de tiempo**: El tiempo total es el m√°ximo de ambas, no la suma
4. **Manejo de errores**: `return_exceptions=True` permite que una b√∫squeda falle sin afectar la otra

### üìä Beneficios

- **Reducci√≥n de latencia**: Si cada b√∫squeda toma 100ms, en paralelo toma ~100ms en lugar de 200ms
- **Mejor rendimiento**: Aprovecha mejor los recursos del servidor de b√∫squeda (Qdrant)
- **Resiliencia**: Si una b√∫squeda falla, la otra puede seguir funcionando

### üîß C√≥mo Funciona

```python
import asyncio

# ‚ùå Evitar (secuencial)
async def search_sequential():
    dense_results = await dense_search(query)    # 100ms
    sparse_results = await sparse_search(query)   # 100ms
    # Total: 200ms

# ‚úÖ Preferir (paralelo)
async def search_parallel():
    dense_results, sparse_results = await asyncio.gather(
        dense_search(query),   # 100ms
        sparse_search(query)   # 100ms
    )
    # Total: ~100ms (el m√°ximo de ambas)
```

---

## 5. Cach√© con Redis

### üìç Ubicaci√≥n
- **Archivo**: `backend/src/core/cache.py`
- **Uso**: Decorador `@cache_result` en m√∫ltiples herramientas

### üéØ Objetivo
Almacenar datos frecuentemente accedidos (resultados RAG, operaciones de red, etc.) en memoria para acceso r√°pido.

### üí° Implementaci√≥n

```215:263:backend/src/core/cache.py
def cache_result(prefix: str, ttl: int = 3600):
    """
    Decorador para cachear resultados de funciones.
    
    Args:
        prefix: Prefijo para las claves de cache
        ttl: Tiempo de vida en segundos
    
    Ejemplo:
        @cache_result("rag", ttl=3600)
        def query(text: str):
            # ... l√≥gica ...
            return result
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_manager = get_cache_manager()
            
            # Si el cache est√° deshabilitado, ejecutar funci√≥n directamente
            if not cache_manager.enabled:
                return func(*args, **kwargs)
            
            # Generar clave de cache √∫nica basada en argumentos
            cache_key = cache_manager.get_cache_key(prefix, *cache_args, **kwargs)
            
            # Intentar obtener del cache
            cached = cache_manager.get(cache_key)
            if cached is not None:
                logger.info(f"Cache HIT: {prefix}")
                return cached
            
            # Cache miss: ejecutar funci√≥n y almacenar resultado
            result = func(*args, **kwargs)
            
            # Almacenar en cache (solo si no hay error)
            if result and not (isinstance(result, dict) and result.get("error")):
                cache_manager.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

### üîë Caracter√≠sticas Clave

1. **Almacenamiento en memoria**: Acceso ultra-r√°pido (< 1ms)
2. **Serializaci√≥n JSON**: Maneja objetos complejos autom√°ticamente
3. **Manejo de errores robusto**: Fallback graceful si Redis no est√° disponible
4. **TTL configurable**: Cada funci√≥n puede tener su propio tiempo de vida
5. **Claves √∫nicas**: Hash MD5 de argumentos para evitar colisiones

### üìä Uso en el Proyecto

```python
# RAG Tool - Cache por 1 hora
@cache_result("rag", ttl=3600)
def query(self, query_text: str, ...):
    # ...

# IP Tool - Cache por 5 minutos (operaciones de red cambian frecuentemente)
@cache_result("ip_ping", ttl=300)
def ping(self, host: str, ...):
    # ...

# IP Tool - Cache por 10 minutos (traceroute)
@cache_result("ip_traceroute", ttl=600)
def tracert(self, host: str, ...):
    # ...
```

### üìä Beneficios

- **Acceso r√°pido**: Redis t√≠picamente responde en < 1ms vs 10-50ms de BD
- **Reducci√≥n de carga**: Menos consultas a Qdrant y menos llamadas a APIs externas
- **Mejor escalabilidad**: Puede manejar miles de lecturas por segundo
- **Ahorro de costos**: Reduce llamadas a OpenAI API para consultas repetidas

---

## 6. Pre-compilaci√≥n de Prompts

### üìç Ubicaci√≥n
- **Archivo**: `backend/src/tools/rag_tool.py` (l√≠neas 34-100)

### üéØ Objetivo
Definir prompts como constantes de clase en lugar de construirlos en cada llamada, reduciendo overhead de procesamiento.

### üí° Implementaci√≥n

```34:100:backend/src/tools/rag_tool.py
class RAGTool:
    """
    Herramienta RAG optimizada con b√∫squeda h√≠brida paralela usando asyncio.gather().
    """
    
    # OPTIMIZACI√ìN: Pre-compilar prompts est√°ticos para evitar reconstruirlos en cada llamada
    RELEVANCE_CHECK_PROMPT_TEMPLATE = """
Analiza si la siguiente pregunta est√° relacionada EXCLUSIVAMENTE con la tem√°tica de redes, telecomunicaciones, protocolos de red, tecnolog√≠as de red, o temas t√©cnicos de TI relacionados con redes.

Pregunta del usuario: "{query_text}"

INSTRUCCIONES CR√çTICAS:
- S√© MUY ESTRICTO: solo marca como relevante si la pregunta est√° CLARAMENTE y DIRECTAMENTE relacionada con redes, protocolos de red, telecomunicaciones o tecnolog√≠as de red
...
"""
    
    COMPLEXITY_PROMPT_TEMPLATE = """
Analiza la siguiente pregunta y determina su complejidad:
...
"""
    
    BASE_PROMPT_TEMPLATE = """
Eres un asistente experto en redes y telecomunicaciones. Responde la pregunta del usuario de manera clara, natural y adaptada a su complejidad.
...
"""
```

### üîë Caracter√≠sticas Clave

1. **Constantes de clase**: Los prompts se definen una vez como atributos de clase
2. **Formateo simple**: Solo se reemplazan variables espec√≠ficas cuando se necesitan
3. **Sin overhead**: No se reconstruyen strings en cada llamada

### üìä Beneficios

- **Reducci√≥n de latencia**: Evita el tiempo de construir strings largos (~5-10ms)
- **Menor uso de CPU**: No necesita procesar templates repetidamente
- **Menor uso de memoria**: Los prompts se comparten entre todas las instancias

### üîß Comparaci√≥n

```python
# ‚ùå Evitar (construcci√≥n en cada uso)
def process_query(query):
    prompt = f"""
    Eres un asistente experto...
    Pregunta: {query}
    ...
    """  # Construye el string cada vez
    return llm.generate(prompt)

# ‚úÖ Preferir (pre-compilaci√≥n)
class QueryProcessor:
    PROMPT_TEMPLATE = """
    Eres un asistente experto...
    Pregunta: {query}
    ...
    """  # Definido una vez
    
    def process_query(self, query):
        prompt = self.PROMPT_TEMPLATE.format(query=query)  # Solo formatea
        return llm.generate(prompt)
```

---

## 7. Validaci√≥n y An√°lisis en Paralelo

### üìç Ubicaci√≥n
- **Archivo**: `backend/src/tools/rag_tool.py` (l√≠neas 395-479)

### üéØ Objetivo
Ejecutar validaci√≥n de relevancia tem√°tica y an√°lisis de complejidad simult√°neamente en lugar de secuencialmente.

### üí° Implementaci√≥n

```395:479:backend/src/tools/rag_tool.py
        # OPTIMIZACI√ìN: Validar relevancia y analizar complejidad en paralelo usando asyncio.gather()
        # IMPORTANTE: La validaci√≥n de relevancia debe considerar el contexto de conversaci√≥n
        async def check_relevance():
            """Verifica si la pregunta es relevante para la tem√°tica."""
            # ... l√≥gica de validaci√≥n ...
        
        async def analyze_complexity():
            """Analiza la complejidad de la pregunta."""
            # ... l√≥gica de an√°lisis ...
        
        # OPTIMIZACI√ìN: Ejecutar validaci√≥n de relevancia y complejidad en paralelo usando asyncio.gather()
        is_relevant, complexity = await asyncio.gather(
            check_relevance(),
            analyze_complexity(),
            return_exceptions=True
        )
```

### üîë Caracter√≠sticas Clave

1. **Ejecuci√≥n paralela**: Ambas operaciones se ejecutan simult√°neamente
2. **Independencia**: La validaci√≥n y el an√°lisis no dependen una de otra
3. **Manejo de errores**: `return_exceptions=True` permite que una falle sin afectar la otra

### üìä Beneficios

- **Reducci√≥n de latencia**: Si cada operaci√≥n toma 200ms, en paralelo toma ~200ms en lugar de 400ms
- **Mejor utilizaci√≥n de recursos**: Aprovecha mejor las llamadas a la API de OpenAI
- **Respuestas m√°s r√°pidas**: El usuario recibe la respuesta m√°s r√°pido

---

## 8. Optimizaciones de Configuraci√≥n

### üìç Ubicaci√≥n
- **Archivo**: `backend/src/models/database.py` (pool de conexiones)
- **Archivo**: `backend/src/core/cache.py` (configuraci√≥n Redis)
- **Archivo**: `backend/src/tools/rag_tool.py` (configuraci√≥n OpenAI client)

### üéØ Objetivo
Ajustar configuraciones del sistema para mejor rendimiento.

### üí° Implementaciones

#### Pool de Conexiones PostgreSQL

```64:73:backend/src/models/database.py
engine = create_engine(
    settings.sqlalchemy_url,
    pool_pre_ping=True,  # Verifica conexiones antes de usarlas
    pool_size=10,  # N√∫mero de conexiones a mantener en el pool
    max_overflow=20,  # M√°ximo de conexiones adicionales que se pueden crear
    pool_recycle=3600,  # Reciclar conexiones despu√©s de 1 hora
    echo=False
)
```

#### Cliente OpenAI con Retries Limitados

```19:22:backend/src/tools/rag_tool.py
client = OpenAI(
    api_key=settings.openai_api_key,
    max_retries=2  # Limitar retries para reducir tiempo de bloqueo
)
```

#### Configuraci√≥n Redis

```58:63:backend/src/core/cache.py
_redis_client = redis.from_url(
    settings.redis_url,
    decode_responses=True,
    socket_connect_timeout=2,  # Timeout corto para conexi√≥n
    socket_timeout=2  # Timeout corto para operaciones
)
```

### üîë Consideraciones

1. **Balance**: M√°s conexiones permiten m√°s concurrencia pero consumen m√°s recursos
2. **Ajuste seg√∫n carga**: Monitorear y ajustar seg√∫n el uso real
3. **Timeouts**: Configurar timeouts apropiados para evitar bloqueos
4. **Retries**: Limitar retries para evitar latencia excesiva

### üìä Beneficios

- **Mejor manejo de carga**: M√°s conexiones permiten m√°s solicitudes concurrentes
- **Prevenci√≥n de cuellos de botella**: Evita que las solicitudes esperen por conexiones
- **Resiliencia**: Timeouts y retries limitados previenen bloqueos prolongados

---

## üìä Resumen de Impacto

| Optimizaci√≥n | Reducci√≥n de Latencia Estimada | Complejidad de Implementaci√≥n | Estado |
|-------------|--------------------------------|-------------------------------|--------|
| Streaming (SSE) | 50-90% (percepci√≥n) | Media | ‚úÖ Implementado |
| Pool de Conexiones | 50-200ms por request | Baja | ‚úÖ Implementado |
| Procesamiento As√≠ncrono | 30-70% en operaciones I/O | Media | ‚úÖ Implementado |
| B√∫squeda H√≠brida Paralela | 50% (de 200ms a 100ms) | Baja | ‚úÖ Implementado |
| Cach√© Redis | 90-95% (de 50ms a <1ms) | Media | ‚úÖ Implementado |
| Pre-compilaci√≥n de Prompts | 5-10ms por request | Baja | ‚úÖ Implementado |
| Validaci√≥n y An√°lisis Paralelo | 50% (de 400ms a 200ms) | Baja | ‚úÖ Implementado |
| Optimizaciones de Configuraci√≥n | Variable | Baja | ‚úÖ Implementado |

---

## üéØ Optimizaciones Implementadas

### ‚úÖ Todas las Optimizaciones Principales

1. ‚úÖ **Streaming (SSE)**: Implementado en `backend/src/api/streaming.py`
2. ‚úÖ **Pool de Conexiones**: Implementado en `backend/src/models/database.py`
3. ‚úÖ **Procesamiento As√≠ncrono**: Todo el proyecto usa `async/await`
4. ‚úÖ **B√∫squeda H√≠brida Paralela**: Implementado en `backend/src/tools/rag_tool.py`
5. ‚úÖ **Cach√© Redis**: Implementado en `backend/src/core/cache.py` con decorador `@cache_result`
6. ‚úÖ **Pre-compilaci√≥n de Prompts**: Implementado en `backend/src/tools/rag_tool.py`
7. ‚úÖ **Validaci√≥n y An√°lisis Paralelo**: Implementado en `backend/src/tools/rag_tool.py`
8. ‚úÖ **Optimizaciones de Configuraci√≥n**: Pool de conexiones, timeouts, retries limitados

---

## üîç M√©tricas para Monitorear

Para validar las optimizaciones en el proyecto:

1. **Time to First Token (TTFT)**: Tiempo hasta el primer token en streaming
2. **Time to Last Token (TTLT)**: Tiempo total de generaci√≥n
3. **Latencia de BD**: Tiempo promedio de queries a PostgreSQL
4. **Hit Rate de Cach√©**: Porcentaje de hits en Redis
5. **Concurrencia**: N√∫mero de solicitudes simult√°neas manejadas
6. **Throughput**: Solicitudes por segundo
7. **Latencia de B√∫squeda**: Tiempo promedio de b√∫squedas en Qdrant

---

## üìö Referencias en el C√≥digo

- **Streaming**: `backend/src/api/streaming.py`
- **Pool de Conexiones**: `backend/src/models/database.py:64-73`
- **B√∫squeda H√≠brida**: `backend/src/tools/rag_tool.py:320-368`
- **Cach√© Redis**: `backend/src/core/cache.py`
- **Pre-compilaci√≥n de Prompts**: `backend/src/tools/rag_tool.py:34-100`
- **Validaci√≥n Paralela**: `backend/src/tools/rag_tool.py:395-479`

---

## üîß Configuraci√≥n de TTLs de Cach√©

Los TTLs (Time To Live) configurados en el proyecto:

- **RAG queries**: 3600 segundos (1 hora) - Consultas conceptuales cambian poco
- **Conversation context**: 1800 segundos (30 minutos) - Contexto de conversaci√≥n
- **IP ping**: 300 segundos (5 minutos) - Operaciones de red cambian frecuentemente
- **IP traceroute**: 600 segundos (10 minutos) - Rutas de red son m√°s estables
- **IP comparison**: 600 segundos (10 minutos) - Comparaciones de IPs

Estos valores pueden ajustarse seg√∫n las necesidades del proyecto.

---

**√öltima actualizaci√≥n**: Basado en an√°lisis del proyecto NetMind v1.0.0
